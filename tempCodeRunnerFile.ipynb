import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder
from sklearn.compose import ColumnTransformer, TransformedTargetRegressor
from sklearn.pipeline import Pipeline
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, median_absolute_error
from typing import Tuple, Dict, Any

# --- CONFIGURATION ---
RANDOM_STATE = 42
DATA_PATH = 'FoAI_A2_data_4.6k.csv'
sns.set_theme(style="whitegrid")

def load_and_clean_data(filepath: str, outcome_col: str = 'salary_in_usd') -> pd.DataFrame:
    """
    Loads data and performs robust outlier removal using the IQR method or Quantiles.
    """
    try:
        df = pd.read_csv(filepath)
    except FileNotFoundError:
        raise FileNotFoundError(f"Could not find file at {filepath}")

    # Remove duplicates if any
    df = df.drop_duplicates()

    # Outlier Removal (Quantile Method - Middle 98%)
    q_low = df[outcome_col].quantile(0.01)
    q_high = df[outcome_col].quantile(0.99)
    
    df_clean = df[(df[outcome_col] > q_low) & (df[outcome_col] < q_high)].copy()
    
    print(f"Original shape: {df.shape}, Cleaned shape: {df_clean.shape}")
    return df_clean

def get_preprocessor() -> ColumnTransformer:
    """
    Defines the feature engineering pipeline.
    Fixes the bug where features were processed twice.
    """
    # 1. Ordinal Features (Order matters)
    ordinal_cols = ['experience_level', 'company_size']
    ordinal_cats = [['EN', 'MI', 'SE', 'EX'], ['S', 'M', 'L']]

    # 2. Nominal Features (Categories without order)
    # NOTE: Removed 'work_year' and 'remote_ratio' from here to avoid duplication
    nominal_cols = ['job_title', 'employee_residence', 'employment_type', 'work_year']

    # 3. Numeric/Passthrough Features
    # Treat remote_ratio and work_year as numeric/passthrough to preserve their magnitude trend
    numeric_cols = ['remote_ratio']

    return ColumnTransformer(
        transformers=[
            ('ord', OrdinalEncoder(categories=ordinal_cats), ordinal_cols),
            # min_frequency=0.01 groups rare job titles into "infrequent" to reduce noise
            ('nom', OneHotEncoder(handle_unknown='ignore', min_frequency=0.01), nominal_cols),
            ('num', 'passthrough', numeric_cols)
        ],
        remainder='drop' # Drop any columns not explicitly mentioned
    )

def build_model_pipeline(preprocessor: ColumnTransformer) -> Pipeline:
    """
    Creates the pipeline with Log-Target Transformation.
    """
    return Pipeline(steps=[
        ('preprocessor', preprocessor),
        ('model', TransformedTargetRegressor(
            regressor=GradientBoostingRegressor(random_state=RANDOM_STATE),
            func=np.log1p,       # Log(y + 1)
            inverse_func=np.expm1 # Exp(y) - 1
        ))
    ])

def train_and_tune(X_train: pd.DataFrame, y_train: pd.Series, pipeline: Pipeline) -> Any:
    """
    Performs Hyperparameter Tuning using RandomizedSearchCV.
    """
    # Note the double underscore syntax: model__regressor__param_name
    param_dist = {
        'model__regressor__n_estimators': [300, 500, 700],
        'model__regressor__learning_rate': [0.01, 0.05, 0.1],
        'model__regressor__max_depth': [3, 4, 5],
        'model__regressor__subsample': [0.7, 0.8],
        'model__regressor__min_samples_leaf': [2, 4]
    }

    print("--- Starting Hyperparameter Tuning ---")
    search = RandomizedSearchCV(
        pipeline,
        param_distributions=param_dist,
        n_iter=20, 
        cv=5, 
        scoring='neg_mean_absolute_error',
        n_jobs=-1, # Use all available CPU cores
        random_state=RANDOM_STATE,
        verbose=1
    )
    
    search.fit(X_train, y_train)
    return search.best_estimator_, search.best_params_

def evaluate_model(model: Any, X_test: pd.DataFrame, y_test: pd.Series):
    """
    Calculates metrics and plots Actual vs Predicted.
    """
    y_pred = model.predict(X_test)

    # Metrics
    mae = mean_absolute_error(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    medae = median_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    print(f"\n--- EVALUATION METRICS ---")
    print(f"MAE  (Mean Abs Error):   ${mae:,.2f}")
    print(f"RMSE (Root Mean Sq):     ${rmse:,.2f}")
    print(f"MedAE (Median Error):    ${medae:,.2f}")
    print(f"R2 Score:                {r2:.4f}")

    # Visualization
    plt.figure(figsize=(10, 6))
    sns.scatterplot(x=y_test, y=y_pred, alpha=0.6, edgecolor=None, color='#2c3e50')
    
    # Perfect fit line
    min_val, max_val = y_test.min(), y_test.max()
    plt.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect Fit')
    
    plt.title(f'Actual vs Predicted Salary (MAE: ${mae:,.0f})')
    plt.xlabel('Actual Salary (USD)')
    plt.ylabel('Predicted Salary (USD)')
    plt.legend()
    plt.tight_layout()
    plt.show()

# --- MAIN EXECUTION ---
if __name__ == "__main__":
    # 1. Load Data
    df = load_and_clean_data(DATA_PATH)

    # 2. Split Features and Target
    X = df.drop(columns=['salary', 'salary_currency', 'salary_in_usd'])
    y = df['salary_in_usd']

    # 3. Train/Test Split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=RANDOM_STATE
    )

    # 4. Build & Train
    preprocessor = get_preprocessor()
    pipeline = build_model_pipeline(preprocessor)
    
    best_model, best_params = train_and_tune(X_train, y_train, pipeline)
    print(f"\nBest Parameters: {best_params}")

    # 5. Evaluate
    evaluate_model(best_model, X_test, y_test)